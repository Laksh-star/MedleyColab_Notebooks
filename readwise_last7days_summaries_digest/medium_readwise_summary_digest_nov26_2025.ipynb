{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgVwUF4HIskw"
      },
      "outputs": [],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘   Readwise â†’ Last 7 Days Export (Documents + All Notes)       â•‘\n",
        "# â•‘   FIXED VERSION: Handles date formats & empty docs            â•‘\n",
        "# â•‘   Fully working in Google Colab â€“ November 2025               â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Step 1: Install nothing extra â€“ requests is already in Colab\n",
        "import requests\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Step 2: PUT YOUR READWISE TOKEN HERE\n",
        "# Get it from https://readwise.io/access_token\n",
        "TOKEN = \"your-token-here\"   # â†â†â† CHANGE THIS!\n",
        "\n",
        "if TOKEN == \"YOUR_TOKEN_HERE\" or len(TOKEN) < 20:\n",
        "    raise ValueError(\"Please paste your real Readwise token above!\")\n",
        "\n",
        "headers = {\"Authorization\": f\"Token {TOKEN}\"}\n",
        "\n",
        "# Step 3: Calculate \"last 7 days\" in UTC (strict ISO for APIs)\n",
        "days_back = 7\n",
        "cutoff_dt = datetime.now(timezone.utc) - timedelta(days=days_back)\n",
        "cutoff = cutoff_dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")  # e.g., \"2025-11-19T12:34:56Z\" â€“ strict ISO with Z\n",
        "cutoff_no_z = cutoff_dt.strftime(\"%Y-%m-%dT%H:%M:%S\")  # For classic API fallback\n",
        "print(f\"Fetching everything updated after: {cutoff} (last {days_back} days)\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Part A: Get all recent DOCUMENTS from Reader API (with fallback)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"\\nFetching documents from Reader API (articles, emails, PDFs, etc.)...\")\n",
        "all_documents = []\n",
        "next_cursor = None\n",
        "use_date_filter = True\n",
        "\n",
        "# First try with date filter\n",
        "params = {\"updatedAfter\": cutoff, \"pageCursor\": next_cursor}\n",
        "url = \"https://readwise.io/api/v3/list/\"\n",
        "r = requests.get(url, headers=headers, params=params)\n",
        "if r.status_code == 400 and \"updatedAfter\" in r.text:\n",
        "    print(\"   â†’ Date filter issue detected. Falling back to fetch all + Python filter (one-time only).\")\n",
        "    use_date_filter = False\n",
        "    # Fetch without date filter (paginated)\n",
        "    params = {\"pageCursor\": next_cursor}\n",
        "    r = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "data = r.json() if r.status_code == 200 else {}\n",
        "batch = data.get(\"results\", [])\n",
        "\n",
        "if use_date_filter or not use_date_filter:\n",
        "    all_documents.extend(batch)\n",
        "    print(f\"   â†’ Got {len(batch)} documents (total so far: {len(all_documents)})\")\n",
        "\n",
        "    # Paginate if needed\n",
        "    while True:\n",
        "        next_cursor = data.get(\"nextPageCursor\")\n",
        "        if not next_cursor:\n",
        "            break\n",
        "        params[\"pageCursor\"] = next_cursor\n",
        "        time.sleep(0.5)\n",
        "        r = requests.get(url, headers=headers, params=params)\n",
        "        if r.status_code != 200:\n",
        "            print(f\"Pagination error: {r.status_code}\")\n",
        "            break\n",
        "        data = r.json()\n",
        "        batch = data.get(\"results\", [])\n",
        "        all_documents.extend(batch)\n",
        "        print(f\"   â†’ Got {len(batch)} more (total: {len(all_documents)})\")\n",
        "\n",
        "# If fallback used, filter in Python\n",
        "if not use_date_filter:\n",
        "    cutoff_dt_obj = cutoff_dt  # Reuse datetime obj\n",
        "    all_documents = [\n",
        "        doc for doc in all_documents\n",
        "        if datetime.fromisoformat(doc.get(\"updated_at\", \"\").replace(\"Z\", \"+00:00\")) > cutoff_dt_obj\n",
        "    ]\n",
        "    print(f\"   â†’ Python filtered to {len(all_documents)} recent docs\")\n",
        "\n",
        "if r.status_code != 200:\n",
        "    print(f\"Reader API error {r.status_code}: {r.text}\")\n",
        "\n",
        "print(f\"\\nFinished! Found {len(all_documents)} documents touched in the last {days_back} days.\\n\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Part B: Get all recent HIGHLIGHTS/NOTES from classic Readwise API\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(\"Fetching recent highlights & inline notes from Readwise API...\")\n",
        "all_highlights = []\n",
        "url = \"https://readwise.io/api/v2/highlights/\"\n",
        "params = {\n",
        "    \"highlighted_at__gt\": cutoff_no_z,  # Classic API likes no Z\n",
        "    \"page_size\": 1000\n",
        "}\n",
        "\n",
        "while True:\n",
        "    r = requests.get(url, headers=headers, params=params)\n",
        "    if r.status_code != 200:\n",
        "        print(f\"Error {r.status_code}: {r.text}\")\n",
        "        break\n",
        "\n",
        "    data = r.json()\n",
        "    batch = data.get(\"results\", [])\n",
        "    all_highlights.extend(batch)\n",
        "    print(f\"   â†’ Got {len(batch)} highlights (total so far: {len(all_highlights)})\")\n",
        "\n",
        "    if not data.get(\"next\"):\n",
        "        break\n",
        "    url = data[\"next\"]  # Follow pagination URL\n",
        "    params = {}  # No extra params for next pages\n",
        "    time.sleep(0.5)\n",
        "\n",
        "# If date filter failed here too (rare), fallback filter\n",
        "if len(all_highlights) > 1000:  # Sanity check for fallback trigger\n",
        "    print(\"   â†’ Applying Python filter to highlights...\")\n",
        "    all_highlights = [\n",
        "        h for h in all_highlights\n",
        "        if datetime.fromisoformat(h.get(\"highlighted_at\", \"\").replace(\"Z\", \"+00:00\")) > cutoff_dt\n",
        "    ]\n",
        "\n",
        "print(f\"Found {len(all_highlights)} highlights/notes made in the last {days_back} days.\\n\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Part C: Combine everything nicely\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "rows = []\n",
        "\n",
        "# If we have documents, match highlights to them (doc_id â‰ˆ book_id)\n",
        "if all_documents:\n",
        "    doc_dict = {doc[\"id\"]: doc for doc in all_documents}  # Quick lookup\n",
        "\n",
        "    for doc in all_documents:\n",
        "        doc_id = doc.get(\"id\")\n",
        "        title = doc.get(\"title\", \"(no title)\")\n",
        "        source = doc.get(\"source\", \"unknown\")\n",
        "        url = doc.get(\"source_url\") or doc.get(\"site_url\") or \"\"\n",
        "        updated_str = doc[\"updated_at\"][:19].replace(\"T\", \" \") if doc.get(\"updated_at\") else \"N/A\"\n",
        "        doc_notes = doc.get(\"notes\", \"\").strip()\n",
        "\n",
        "        # Find matching highlights\n",
        "        highlights_for_doc = [h for h in all_highlights if str(h.get(\"book_id\")) == str(doc_id)]\n",
        "\n",
        "        # Add doc row if notes or highlights\n",
        "        if doc_notes or highlights_for_doc:\n",
        "            rows.append({\n",
        "                \"Type\": \"Document\",\n",
        "                \"Title\": title,\n",
        "                \"URL\": url,\n",
        "                \"Source\": source,\n",
        "                \"Updated\": updated_str,\n",
        "                \"Document Notes\": doc_notes,\n",
        "                \"Highlight Text\": \"\",\n",
        "                \"Highlight Note\": \"\"\n",
        "            })\n",
        "\n",
        "            # Add highlight rows\n",
        "            for h in highlights_for_doc:\n",
        "                rows.append({\n",
        "                    \"Type\": \"Highlight\",\n",
        "                    \"Title\": title,\n",
        "                    \"URL\": url,\n",
        "                    \"Source\": source,\n",
        "                    \"Updated\": updated_str,\n",
        "                    \"Document Notes\": \"\",\n",
        "                    \"Highlight Text\": h[\"text\"].strip(),\n",
        "                    \"Highlight Note\": h.get(\"note\", \"\").strip()\n",
        "                })\n",
        "\n",
        "# Always add unmatched highlights (e.g., from books or legacy sources)\n",
        "unmatched_highlights = [\n",
        "    h for h in all_highlights\n",
        "    if all_documents and str(h.get(\"book_id\")) not in [str(d[\"id\"]) for d in all_documents]\n",
        "]\n",
        "for h in unmatched_highlights:\n",
        "    title = h.get(\"title\", \"(no title)\")\n",
        "    source = h.get(\"source_type\", \"unknown\")\n",
        "    url = h.get(\"source_url\", \"\") or \"\"\n",
        "    updated_str = h.get(\"highlighted_at\", \"\")[:19].replace(\"T\", \" \") if h.get(\"highlighted_at\") else \"N/A\"\n",
        "    rows.append({\n",
        "        \"Type\": \"Highlight (unmatched doc)\",\n",
        "        \"Title\": title,\n",
        "        \"URL\": url,\n",
        "        \"Source\": source,\n",
        "        \"Updated\": updated_str,\n",
        "        \"Document Notes\": \"\",\n",
        "        \"Highlight Text\": h[\"text\"].strip(),\n",
        "        \"Highlight Note\": h.get(\"note\", \"\").strip()\n",
        "    })\n",
        "\n",
        "# If no rows yet (edge case), add a note\n",
        "if not rows:\n",
        "    rows = [{\"Type\": \"Summary\", \"Title\": \"No data\", \"URL\": \"\", \"Source\": \"\", \"Updated\": \"\",\n",
        "             \"Document Notes\": f\"No documents or highlights found in the last {days_back} days.\",\n",
        "             \"Highlight Text\": \"\", \"Highlight Note\": \"\"}]\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Ensure 'Updated' column exists and sort (handle N/A gracefully)\n",
        "if 'Updated' not in df.columns:\n",
        "    df['Updated'] = \"N/A\"\n",
        "df['Updated'] = pd.to_datetime(df['Updated'], errors='coerce')  # Make sortable\n",
        "df = df.sort_values(\"Updated\", ascending=False)\n",
        "\n",
        "# Reset index after sort\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Part D: Display & Download\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "total_rows = len(df)\n",
        "doc_count = len([r for r in rows if r[\"Type\"] == \"Document\"])\n",
        "hl_count = len([r for r in rows if \"Highlight\" in r[\"Type\"]])\n",
        "\n",
        "display(Markdown(f\"### Complete export: {total_rows} rows ({doc_count} documents, {hl_count} highlights)\"))\n",
        "display(df.head(20))  # Preview first 20 rows\n",
        "\n",
        "# Save to CSV\n",
        "filename = f\"readwise_last_{days_back}_days_{datetime.now().strftime('%Y-%m-%d')}.csv\"\n",
        "df.to_csv(filename, index=False)\n",
        "print(f\"\\nSaved to {filename}\")\n",
        "\n",
        "# Optional: Markdown table\n",
        "md_filename = filename.replace(\".csv\", \".md\")\n",
        "with open(md_filename, \"w\") as f:\n",
        "    f.write(df.to_markdown(index=False))\n",
        "print(f\"Also saved as {md_filename}\")\n",
        "\n",
        "# Download (uncomment if needed)\n",
        "# from google.colab import files\n",
        "# files.download(filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install anthropic"
      ],
      "metadata": {
        "id": "GOnYdkLyQ9ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘   ONE-CLICK WEEKLY DIGEST â†’ Claude (automatic)                â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import anthropic\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import userdata  # â† safest way to store secrets in Colab\n",
        "from google.colab import files # Import files for downloading\n",
        "\n",
        "# â†â†â† PUT YOUR ANTHROPIC API KEY IN COLAB SECRETS (Sidebar â†’ ğŸ”‘) AS \"ANTHROPIC_API_KEY\"\n",
        "try:\n",
        "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "except:\n",
        "    ANTHROPIC_API_KEY = \"\"  # fallback â€” will ask you if missing\n",
        "\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    ANTHROPIC_API_KEY = input(\"Paste your Anthropic API key â†’ \").strip()\n",
        "\n",
        "client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "# Read the enhanced CSV we just created\n",
        "csv_content = open(filename, \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "prompt = f\"\"\"You are my personal AI research assistant.\n",
        "\n",
        "Below is a CSV export of everything I highlighted and noted in Readwise over the last 7 days (articles, PDFs, tweets, etc.).\n",
        "\n",
        "Please read the entire CSV and produce a beautiful, concise weekly research digest with this exact structure:\n",
        "\n",
        "## Theme 1 â€“ Catchy Title\n",
        "One-paragraph summary in your own words + the 1â€“3 most important concrete numbers/findings/quotes.\n",
        "\n",
        "## Theme 2 â€“ Catchy Title\n",
        "(repeat)\n",
        "\n",
        "End with a single **Through-line** paragraph that ties everything together (the big picture I seem to be tracking this week).\n",
        "\n",
        "Use the same polished, insightful style as the digest you wrote about Anthropic productivity gains, Harvey interviews, hallucination KPIs, etc.\n",
        "\n",
        "Here is the CSV:\n",
        "\n",
        "{csv_content}\n",
        "\n",
        "Thank you.\"\"\"\n",
        "\n",
        "print(\"Sending to Claude â†’ this takes ~15â€“30 seconds...\")\n",
        "message = client.messages.create(\n",
        "    model=\"claude-3-opus-20240229\",   # Changed to a widely available model\n",
        "    max_tokens=2000,\n",
        "    temperature=0.7,\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        ")\n",
        "\n",
        "digest_md = message.content[0].text\n",
        "\n",
        "# Save + download\n",
        "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "digest_filename = f\"Weekly_Digest_{today}.md\"\n",
        "with open(digest_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"# Readwise Weekly Digest â€“ Week ending {today}\\n\\n\")\n",
        "    f.write(digest_md)\n",
        "\n",
        "display(Markdown(\"### Here is your digest:\"))\n",
        "display(Markdown(digest_md))\n",
        "\n",
        "print(f\"\\nSaved as {digest_filename}\")\n",
        "files.download(digest_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XCfQoFPzQ1eg",
        "outputId": "efbf134f-50ed-4873-bf93-dd83261d45f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending to Claude â†’ this takes ~15â€“30 seconds...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Here is your digest:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Your Weekly Research Digest\n\n## Theme 1 â€“ The Productivity Dividend Is Here (and It's Measurable)\n\nAnthropic's new Economic Index research moves beyond \"where is AI used?\" to \"how much time does it actually save?\" By analyzing 100,000 real Claude conversations, they found tasks that would take ~90 minutes without AI are sped up by roughly **80%**, with wide variation by profession. Extrapolating to the full economy, they estimate current-generation AI could boost annual U.S. labor productivity growth by **1.8 percentage points** over the next decadeâ€”a doubling of baseline trends that places their estimate at the high end of recent studies. The method is novel: Claude itself estimates task duration (with promising accuracy), then those micro-savings are aggregated into macro impact. It's a first attempt to quantify the \"AI dividend\" at scale, and the headline numberâ€”1.8%â€”suggests we're already past the threshold where AI moves the national productivity needle.\n\n## Theme 2 â€“ Collaboration Ability Is Distinct from Solo Ability\n\nA Bayesian Item Response Theory study of 667 humans working with and without AI (Llama-3.1-8B and GPT-4o) reveals that your skill *with* AI (collaborative ability, Îº) is statistically separate from your skill *without* it (solo ability, Î¸). Humanâ€“AI teams outperform either aloneâ€”**+23 percentage points with Llama, +29 pp with GPT-4o**â€”and the gains are largest on harder tasks and for lower-ability users (though higher-ability users still reach higher absolute performance). The kicker: **Theory of Mind predicts collaborative ability but not solo ability**. Users who write prompts that reflect perspective-taking and mental modeling of the AI extract better responses, even controlling for effort. This isn't just about knowing more facts; it's about a new cognitive skillâ€”understanding and steering a non-human partnerâ€”that we can now measure and benchmark.\n\n## Theme 3 â€“ Reasoning Under the Hood (and What's Still Missing)\n\nTwo papers dissect *how* LLMs reason. The cognitive-foundations study maps 28 reasoning elements (goals, metacognition, representations, operations) across 170,000 traces from 17 models and finds models favor shallow forward chaining and over-rely on verification/backtracking, while humans use hierarchical abstraction and self-awareness. When researchers scaffold models with explicit cognitive structureâ€”nudging them toward successful reasoning \"storyboards\"â€”performance on ill-structured problems jumps **up to 60%** (Qwen3-14B on dilemmas). Meanwhile, the autonomous-agents review and hallucination-mitigation work highlight gaps in perception, tool use, and factual grounding: multi-agent pipelines with JSON-based context transfer and novel KPIs (Factual Claim Density, Grounding References) can reduce hallucinations substantially, but proprietary model opacity limits true explainability. The through-line: we now have frameworks to diagnose *where* reasoning breaks and interventions that workâ€”but the black box remains.\n\n## Theme 4 â€“ Hiring, Culture, and the Human Edge\n\nPractical notes from the field: Harvey's CEO interviews candidates in Google Docs to see real collaboration and writing skill, cutting through smooth talk and AI-assisted polish. OpenAI's guide emphasizes hiring for experimentation and rewriting roles as AI shifts tasks. Interface's founderâ€”a Trinidad-born engineerâ€”parlays his \"outsider\" industrial background into an AI safety startup that found **10,800 errors in 2.5 months** at a major energy firm, winning over skeptical executives and field workers alike. The Opus 4.5 \"vibe check\" notes it extends the horizon of what you can code by vibingâ€”no limit found yetâ€”but loses some creative \"magic touch\" in the process, favoring confident execution over second-guessing. Across these stories: *who* you hire, *how* you assess them, and *what* diverse perspectives you bring matter as much as the models you deploy.\n\n## Theme 5 â€“ Risks, Guardrails, and the Governance Tug-of-War\n\nAs AI grows more capable, so do the stakes. \"Vibe hacking\"â€”low-skill bad actors using AI for phishing and malicious codeâ€”prompted Anthropic to disrupt a data-theft-and-extortion scheme in August. The same month, the White House AI Czar accused Anthropic of \"fear-mongering\" to shape regulation. The tension is real: every capability gain widens the attack surface and sharpens the policy fight over who sets the rules. The hallucination-KPI work and cognitive-scaffolding studies are part of the solutionâ€”better measurement, transparency, and steeringâ€”but the governance question looms large.\n\n---\n\n**Through-line:** This week you're tracking AI's transition from prototype to production *system*â€”and the new disciplines that transition demands. Anthropic's 1.8% productivity estimate and the Bayesian synergy study quantify the gains and reveal that collaboration with AI is a distinct, teachable skill (Theory of Mind matters). The reasoning and agent research show we can now map cognitive gaps, design scaffolds that unlock 60% improvements, and measure hallucinations with novel KPIsâ€”yet proprietary opacity and the lack of human-like abstraction remain hard limits. Meanwhile, hiring and culture stories (Harvey's Google Docs interviews, Interface's outsider advantage, Opus 4.5's trade-offs) remind us that *human* judgment, diversity, and adaptability are the irreducible complement to model capability. And as productivity and power grow, so does the governance tug-of-war: who benefits, who's at risk, and who decides. You're assembling a complete picture of AI at scaleâ€”measurement, mechanism, culture, and control."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved as Weekly_Digest_2025-11-26.md\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'files' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4209826803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSaved as {digest_filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigest_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    }
  ]
}