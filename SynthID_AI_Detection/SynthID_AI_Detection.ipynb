{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynthID Detection: Text & Image AI Detection Guide\n",
    "\n",
    "This notebook covers:\n",
    "1. **SynthID Text Detection** (Open Source - Fully Working)\n",
    "2. **SynthID Image Detection** (Limited - Google Cloud Only)\n",
    "3. **Alternative AI Image Detection Methods**\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: SynthID Text Detection (Open Source)\n",
    "\n",
    "SynthID Text is fully open-sourced and available through Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers>=4.46.0 torch -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Watermarked Text with SynthID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    SynthIDTextWatermarkingConfig,\n",
    "    SynthIDTextWatermarkDetector,\n",
    "    SynthIDTextWatermarkLogitsProcessor\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Load a small model for demonstration (you can use larger models)\n",
    "model_name = \"facebook/opt-1.3b\"  # or \"gpt2\", \"meta-llama/Llama-2-7b-hf\", etc.\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Configure SynthID watermarking\n",
    "watermark_config = SynthIDTextWatermarkingConfig(\n",
    "    keys=[654, 400, 836, 123, 340],  # Random keys - KEEP THESE SECRET in production!\n",
    "    ngram_len=5,  # Balance between detectability and robustness\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate watermarked text\n",
    "prompt = \"Artificial intelligence is revolutionizing\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate WITH watermark\n",
    "watermarked_output = model.generate(\n",
    "    **inputs,\n",
    "    watermarking_config=watermark_config,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WATERMARKED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "print(watermarked_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WITHOUT watermark (for comparison)\n",
    "non_watermarked_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "non_watermarked_text = tokenizer.decode(non_watermarked_output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NON-WATERMARKED TEXT:\")\n",
    "print(\"=\"*50)\n",
    "print(non_watermarked_text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Detect SynthID Watermark in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the detector\n",
    "detector = SynthIDTextWatermarkDetector(\n",
    "    watermarking_config=watermark_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Detect watermark in the generated text\n",
    "def detect_watermark(text, label=\"\"):\n",
    "    result = detector(text)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DETECTION RESULT: {label}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(f\"\\nWatermark Detected: {result['prediction']}\")\n",
    "    print(f\"Confidence Score: {result['score']:.4f}\")\n",
    "    \n",
    "    # Interpret the result\n",
    "    if result['prediction'] == 'watermarked':\n",
    "        print(\"‚úÖ This text likely contains a SynthID watermark\")\n",
    "    elif result['prediction'] == 'not_watermarked':\n",
    "        print(\"‚ùå This text does NOT contain a SynthID watermark\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Uncertain - not enough confidence to determine\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test on watermarked text\n",
    "detect_watermark(watermarked_text, \"Watermarked Text\")\n",
    "\n",
    "# Test on non-watermarked text\n",
    "detect_watermark(non_watermarked_text, \"Non-Watermarked Text\")\n",
    "\n",
    "# Test on human-written text\n",
    "human_text = \"The quick brown fox jumps over the lazy dog. This is a classic pangram used in typing tests.\"\n",
    "detect_watermark(human_text, \"Human-Written Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test Robustness to Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test watermark detection after minor modifications\n",
    "import re\n",
    "\n",
    "def modify_text(text, modification_type=\"minor\"):\n",
    "    if modification_type == \"minor\":\n",
    "        # Change a few words\n",
    "        modified = text.replace(\"the\", \"a\").replace(\"is\", \"was\")\n",
    "        return modified[:len(text)]\n",
    "    elif modification_type == \"paraphrase\":\n",
    "        # Mild paraphrasing (simulated)\n",
    "        return text.replace(\".\", \", which means that.\").replace(\"and\", \"as well as\")\n",
    "    elif modification_type == \"truncate\":\n",
    "        # Remove last 20% of text\n",
    "        return text[:int(len(text) * 0.8)]\n",
    "    return text\n",
    "\n",
    "print(\"\\n\" + \"#\"*70)\n",
    "print(\"TESTING WATERMARK ROBUSTNESS\")\n",
    "print(\"#\"*70)\n",
    "\n",
    "# Test minor modifications\n",
    "modified_text = modify_text(watermarked_text, \"minor\")\n",
    "detect_watermark(modified_text, \"Minor Modifications\")\n",
    "\n",
    "# Test after truncation\n",
    "truncated_text = modify_text(watermarked_text, \"truncate\")\n",
    "detect_watermark(truncated_text, \"Truncated Text (80%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SynthID Image Detection (Limited Availability)\n",
    "\n",
    "**IMPORTANT:** SynthID for images is NOT open-source. It's only available through:\n",
    "1. **Google Cloud Vertex AI** (for images generated with Imagen)\n",
    "2. **SynthID Detector Portal** (waitlist only - for journalists, researchers)\n",
    "\n",
    "### 2.1 Google Cloud Vertex AI Approach (Requires GCP Account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code requires Google Cloud credentials and Vertex AI API access\n",
    "# You need to:\n",
    "# 1. Set up a GCP project\n",
    "# 2. Enable Vertex AI API\n",
    "# 3. Have proper authentication\n",
    "\n",
    "\"\"\"\n",
    "# EXAMPLE CODE (Won't work without GCP setup)\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.vision_models import Image, ImageGenerationModel\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Generate image with watermark (automatically applied)\n",
    "model = ImageGenerationModel.from_pretrained(\"imagegeneration@006\")\n",
    "response = model.generate_images(\n",
    "    prompt=\"A beautiful sunset over mountains\",\n",
    "    number_of_images=1,\n",
    ")\n",
    "\n",
    "# Images are automatically watermarked with SynthID\n",
    "images = response.images\n",
    "images[0].save(\"watermarked_image.png\")\n",
    "\n",
    "# Verify watermark\n",
    "from google.cloud.aiplatform.vision_models import WatermarkVerificationModel\n",
    "\n",
    "verification_model = WatermarkVerificationModel.from_pretrained(\"watermark-verification@001\")\n",
    "result = verification_model.verify_image(\n",
    "    image=Image.load_from_file(\"watermarked_image.png\")\n",
    ")\n",
    "\n",
    "print(f\"Watermark detected: {result.watermark_detected}\")\n",
    "print(f\"Confidence: {result.confidence}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ö†Ô∏è  SynthID Image Detection is NOT available as open-source.\")\n",
    "print(\"\\nOptions:\")\n",
    "print(\"1. Use Google Cloud Vertex AI (requires GCP account & billing)\")\n",
    "print(\"2. Join SynthID Detector Portal waitlist:\")\n",
    "print(\"   https://deepmind.google/technologies/synthid/\")\n",
    "print(\"3. Use alternative AI image detection methods (see Part 3 below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Alternative AI Image Detection Methods\n",
    "\n",
    "Since SynthID for images isn't publicly available, here are working alternatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check Image Metadata (EXIF/C2PA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow exifread -q\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import exifread\n",
    "\n",
    "def check_image_metadata(image_path):\n",
    "    \"\"\"Check image metadata for AI generation indicators\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ANALYZING: {image_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Using PIL\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        exifdata = img.getexif()\n",
    "        \n",
    "        if exifdata:\n",
    "            print(\"\\nüìã EXIF Data Found:\")\n",
    "            ai_indicators = []\n",
    "            \n",
    "            for tag_id, value in exifdata.items():\n",
    "                tag_name = TAGS.get(tag_id, tag_id)\n",
    "                \n",
    "                # Look for AI-related keywords\n",
    "                value_str = str(value).lower()\n",
    "                if any(keyword in value_str for keyword in \n",
    "                       ['ai', 'artificial', 'generated', 'stable diffusion', \n",
    "                        'midjourney', 'dall-e', 'imagen', 'synthid']):\n",
    "                    ai_indicators.append(f\"{tag_name}: {value}\")\n",
    "                    print(f\"  ‚ö†Ô∏è  {tag_name}: {value}\")\n",
    "            \n",
    "            if ai_indicators:\n",
    "                print(\"\\n‚úÖ AI Generation Indicators Found!\")\n",
    "            else:\n",
    "                print(\"\\n‚ùì No obvious AI indicators in metadata\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå No EXIF data found (metadata stripped or never existed)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error reading image: {e}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Example usage\n",
    "# check_image_metadata('your_image.png')\n",
    "\n",
    "print(\"\\nüí° Note: Many AI image generators strip metadata, making this method unreliable.\")\n",
    "print(\"   This is why pixel-based watermarking (like SynthID) is more robust.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hugging Face AI Image Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers pillow torch torchvision -q\n",
    "\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load AI image detection model\n",
    "print(\"Loading AI image detector...\")\n",
    "detector = pipeline(\"image-classification\", model=\"umm-maybe/AI-image-detector\")\n",
    "\n",
    "def detect_ai_image(image_path):\n",
    "    \"\"\"Detect if an image is AI-generated using Hugging Face model\"\"\"\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    results = detector(img)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"AI IMAGE DETECTION RESULTS: {image_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for result in results:\n",
    "        label = result['label']\n",
    "        score = result['score'] * 100\n",
    "        \n",
    "        if label == 'artificial':\n",
    "            icon = \"ü§ñ\"\n",
    "        else:\n",
    "            icon = \"üë§\"\n",
    "        \n",
    "        print(f\"{icon} {label.upper()}: {score:.2f}%\")\n",
    "    \n",
    "    # Determine verdict\n",
    "    top_prediction = results[0]\n",
    "    if top_prediction['label'] == 'artificial' and top_prediction['score'] > 0.7:\n",
    "        print(\"\\n‚úÖ LIKELY AI-GENERATED\")\n",
    "    elif top_prediction['label'] == 'human' and top_prediction['score'] > 0.7:\n",
    "        print(\"\\n‚úÖ LIKELY HUMAN-CREATED\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  UNCERTAIN - confidence too low\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# detect_ai_image('suspicious_image.png')\n",
    "\n",
    "print(\"\\nModel loaded! Use detect_ai_image('path_to_image.jpg') to analyze images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 OpenAI's CLIP-based Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers pillow -q\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "def detect_ai_with_clip(image_path):\n",
    "    \"\"\"Use CLIP to detect AI-generated images based on visual features\"\"\"\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Prompts that describe AI vs real images\n",
    "    text_prompts = [\n",
    "        \"a computer generated image\",\n",
    "        \"an AI generated digital art\",\n",
    "        \"a photograph taken with a camera\",\n",
    "        \"a real photograph of the world\",\n",
    "        \"artificial intelligence artwork\",\n",
    "        \"authentic photograph\"\n",
    "    ]\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=text_prompts,\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLIP-BASED AI DETECTION: {image_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, prompt in enumerate(text_prompts):\n",
    "        prob = probs[0][i].item() * 100\n",
    "        print(f\"{prompt}: {prob:.2f}%\")\n",
    "    \n",
    "    # Calculate AI vs Real scores\n",
    "    ai_score = (probs[0][0] + probs[0][1] + probs[0][4]).item() * 100 / 3\n",
    "    real_score = (probs[0][2] + probs[0][3] + probs[0][5]).item() * 100 / 3\n",
    "    \n",
    "    print(f\"\\nAggregate Scores:\")\n",
    "    print(f\"ü§ñ AI-Generated: {ai_score:.2f}%\")\n",
    "    print(f\"üì∑ Real Photo: {real_score:.2f}%\")\n",
    "    \n",
    "    if ai_score > real_score:\n",
    "        print(\"\\n‚úÖ More likely AI-GENERATED\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ More likely REAL PHOTOGRAPH\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return probs\n",
    "\n",
    "# Example usage:\n",
    "# detect_ai_with_clip('test_image.jpg')\n",
    "\n",
    "print(\"\\nCLIP model loaded! Use detect_ai_with_clip('image.jpg') to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Statistical Analysis Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy opencv-python pillow scipy -q\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_image_statistics(image_path):\n",
    "    \"\"\"Analyze statistical properties that differ between AI and real images\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"STATISTICAL ANALYSIS: {image_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # 1. Noise analysis\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    noise_estimate = np.std(cv2.Laplacian(gray, cv2.CV_64F))\n",
    "    print(f\"\\nüìä Noise Level: {noise_estimate:.2f}\")\n",
    "    if noise_estimate < 10:\n",
    "        print(\"   ‚ö†Ô∏è  Very low noise (AI images often have minimal noise)\")\n",
    "    \n",
    "    # 2. Color histogram analysis\n",
    "    hist_r = cv2.calcHist([img_rgb], [0], None, [256], [0, 256])\n",
    "    hist_g = cv2.calcHist([img_rgb], [1], None, [256], [0, 256])\n",
    "    hist_b = cv2.calcHist([img_rgb], [2], None, [256], [0, 256])\n",
    "    \n",
    "    # Measure histogram smoothness (AI images often have smoother distributions)\n",
    "    hist_variance = np.var(hist_r) + np.var(hist_g) + np.var(hist_b)\n",
    "    print(f\"\\nüé® Color Distribution Variance: {hist_variance:.2f}\")\n",
    "    if hist_variance > 1000000:\n",
    "        print(\"   ‚ö†Ô∏è  Very smooth color distribution (typical of AI)\")\n",
    "    \n",
    "    # 3. Edge detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_density = np.sum(edges) / edges.size\n",
    "    print(f\"\\nüîç Edge Density: {edge_density:.4f}\")\n",
    "    if edge_density < 0.05:\n",
    "        print(\"   ‚ö†Ô∏è  Low edge density (AI images can have overly smooth edges)\")\n",
    "    \n",
    "    # 4. Compression artifacts\n",
    "    # AI-generated images often lack typical JPEG compression artifacts\n",
    "    dct = cv2.dct(np.float32(gray))\n",
    "    dct_mean = np.mean(np.abs(dct))\n",
    "    print(f\"\\nüì∏ DCT Mean: {dct_mean:.2f}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    ai_indicators = 0\n",
    "    if noise_estimate < 10:\n",
    "        ai_indicators += 1\n",
    "    if hist_variance > 1000000:\n",
    "        ai_indicators += 1\n",
    "    if edge_density < 0.05:\n",
    "        ai_indicators += 1\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"AI Indicators Found: {ai_indicators}/3\")\n",
    "    \n",
    "    if ai_indicators >= 2:\n",
    "        print(\"‚ö†Ô∏è  Statistical properties suggest possible AI generation\")\n",
    "    else:\n",
    "        print(\"‚úÖ Statistical properties more consistent with real photo\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nüí° Note: Statistical analysis is indicative, not definitive.\")\n",
    "    print(\"   Combine with other methods for better accuracy.\")\n",
    "\n",
    "# Example usage:\n",
    "# analyze_image_statistics('test_image.jpg')\n",
    "\n",
    "print(\"\\nStatistical analysis ready! Use analyze_image_statistics('image.jpg')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Recommendations\n",
    "\n",
    "### For TEXT Detection:\n",
    "‚úÖ **Use SynthID Text** (fully open-source)\n",
    "- Most reliable for text generated with watermarking enabled\n",
    "- Available through Hugging Face Transformers\n",
    "\n",
    "### For IMAGE Detection:\n",
    "\n",
    "#### If you have Google Cloud access:\n",
    "1. **SynthID on Vertex AI** (most reliable, but limited to Imagen-generated images)\n",
    "\n",
    "#### If you don't have Google Cloud:\n",
    "1. **Hugging Face AI Detector** (Best free option)\n",
    "2. **CLIP-based Detection** (Good for general assessment)\n",
    "3. **Metadata Check** (Quick but easily defeated)\n",
    "4. **Statistical Analysis** (Supplementary indicator)\n",
    "\n",
    "### Best Practice:\n",
    "**Combine multiple methods** for higher confidence:\n",
    "```python\n",
    "# Comprehensive check\n",
    "check_image_metadata('image.jpg')        # Quick check\n",
    "detect_ai_image('image.jpg')             # ML-based\n",
    "detect_ai_with_clip('image.jpg')         # CLIP-based\n",
    "analyze_image_statistics('image.jpg')    # Statistical\n",
    "```\n",
    "\n",
    "### Limitations:\n",
    "- No detector is 100% accurate\n",
    "- Sophisticated manipulation can fool all methods\n",
    "- SynthID Image is the most robust but not publicly available\n",
    "- Alternative methods give probabilistic estimates, not certainty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
